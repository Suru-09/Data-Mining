{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating a graph using bigrams vs using NER (spacy)\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "from pandas import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk import *\n",
    "from siuba import count\n",
    "\n",
    "%store -r game_text_cleaned\n",
    "%store -r swords_text_cleaned\n",
    "\n",
    "%store -r game_df_cleaned\n",
    "%store -r swords_df_cleaned\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93maveraged_perceptron_tagger\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Iulia/nltk_data'\n    - 'C:\\\\University\\\\year3\\\\Sem2\\\\DM\\\\Project\\\\Data-Mining\\\\venv\\\\nltk_data'\n    - 'C:\\\\University\\\\year3\\\\Sem2\\\\DM\\\\Project\\\\Data-Mining\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\University\\\\year3\\\\Sem2\\\\DM\\\\Project\\\\Data-Mining\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Iulia\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 66>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     62\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(character_list, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCharacter1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCharacter2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistance\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[1;32m---> 66\u001B[0m game_df_bi \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_bigram\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgame_text_cleaned\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mA Game of Thrones\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m game_df_bi\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mcreate_bigram\u001B[1;34m(text, book_name)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_bigram\u001B[39m(text, book_name : \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m---> 60\u001B[0m     character_list \u001B[38;5;241m=\u001B[39m \u001B[43mfind_characters\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbook_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;66;03m# print(character_list)\u001B[39;00m\n\u001B[0;32m     62\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(character_list, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCharacter1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCharacter2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistance\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mfind_characters\u001B[1;34m(text, book_name)\u001B[0m\n\u001B[0;32m     13\u001B[0m not_good_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     14\u001B[0m good_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tag \u001B[38;5;129;01min\u001B[39;00m \u001B[43mpos_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tag[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m verbs:\n\u001B[0;32m     18\u001B[0m         verb_list\u001B[38;5;241m.\u001B[39mappend(tag[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32mC:\\University\\year3\\Sem2\\DM\\Project\\Data-Mining\\venv\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001B[0m, in \u001B[0;36mpos_tag\u001B[1;34m(tokens, tagset, lang)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpos_tag\u001B[39m(tokens, tagset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meng\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    tag the given list of tokens.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;124;03m    :rtype: list(tuple(str, str))\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 165\u001B[0m     tagger \u001B[38;5;241m=\u001B[39m \u001B[43m_get_tagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001B[1;32mC:\\University\\year3\\Sem2\\DM\\Project\\Data-Mining\\venv\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001B[0m, in \u001B[0;36m_get_tagger\u001B[1;34m(lang)\u001B[0m\n\u001B[0;32m    105\u001B[0m     tagger\u001B[38;5;241m.\u001B[39mload(ap_russian_model_loc)\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 107\u001B[0m     tagger \u001B[38;5;241m=\u001B[39m \u001B[43mPerceptronTagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tagger\n",
      "File \u001B[1;32mC:\\University\\year3\\Sem2\\DM\\Project\\Data-Mining\\venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001B[0m, in \u001B[0;36mPerceptronTagger.__init__\u001B[1;34m(self, load)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load:\n\u001B[0;32m    166\u001B[0m     AP_MODEL_LOC \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\n\u001B[1;32m--> 167\u001B[0m         \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mPICKLE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    168\u001B[0m     )\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload(AP_MODEL_LOC)\n",
      "File \u001B[1;32mC:\\University\\year3\\Sem2\\DM\\Project\\Data-Mining\\venv\\lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93maveraged_perceptron_tagger\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Iulia/nltk_data'\n    - 'C:\\\\University\\\\year3\\\\Sem2\\\\DM\\\\Project\\\\Data-Mining\\\\venv\\\\nltk_data'\n    - 'C:\\\\University\\\\year3\\\\Sem2\\\\DM\\\\Project\\\\Data-Mining\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\University\\\\year3\\\\Sem2\\\\DM\\\\Project\\\\Data-Mining\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Iulia\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def find_characters(text : str, book_name : str):\n",
    "    text_str = \"\"\n",
    "    for _list in text:\n",
    "        for string in _list:\n",
    "            text_str += string + ' '\n",
    "\n",
    "    verbs = ['VB', 'VBD', 'VBZ', 'VBG', 'VBP', 'VBN']\n",
    "    not_good = ['NNS', 'PDT', 'FW', 'EX', 'PRP', 'RB', 'RBR', 'RBS', 'UH', 'TO', 'CC', 'CD', 'WDT', 'WP', 'WRB']\n",
    "    good = ['NNP']\n",
    "    not_good += verbs\n",
    "    token = word_tokenize(text_str)\n",
    "    verb_list = []\n",
    "    not_good_list = []\n",
    "    good_list = []\n",
    "\n",
    "    for tag in pos_tag(token):\n",
    "        if tag[1] in verbs:\n",
    "            verb_list.append(tag[0])\n",
    "        elif tag[1] in not_good:\n",
    "            not_good_list.append(tag[0])\n",
    "        elif tag[1] in good:\n",
    "            good_list.append(tag[0])\n",
    "\n",
    "\n",
    "\n",
    "    df_bigrams_ = ngrams(token, 2)\n",
    "    df_bigrams = [ ' '.join(grams) for grams in df_bigrams_]\n",
    "    df = pd.DataFrame({\n",
    "        \"book\" : book_name,\n",
    "        \"bigrams\": df_bigrams\n",
    "    })\n",
    "    df[['word1', 'word2']] = df['bigrams'].str.split(expand=True)\n",
    "    df = df[~df.word1.isin(verb_list)]\n",
    "    df = df[df.word2.isin(verb_list)]\n",
    "    df = df[df.word1.str.istitle()]\n",
    "    df = df[~df.word1.isin(not_good_list)]\n",
    "    df = df[df.word1.isin(good_list)]\n",
    "\n",
    "    var = count(df, 'word1', sort=True)\n",
    "    character_list = []\n",
    "    for character1 in var['word1']:\n",
    "        tmp_chars = []\n",
    "        index_c1 = -1\n",
    "        index_c1 = text_str.find(character1)\n",
    "        if index_c1 >= 0:\n",
    "            for character2 in var['word1']:\n",
    "                if character1 != character2:\n",
    "                    idx_c2 = text_str.find(character2, index_c1, len(text_str) - 1)\n",
    "                    if idx_c2 > index_c1:\n",
    "                        tmp_chars.append((character1, character2, idx_c2 - index_c1))\n",
    "            if tmp_chars:\n",
    "                tmp_chars.sort(reverse=False, key = lambda x : x[2])\n",
    "                character_list += tmp_chars[:2]\n",
    "\n",
    "\n",
    "    return character_list\n",
    "\n",
    "\n",
    "def create_bigram(text, book_name : str):\n",
    "    character_list = find_characters(text, book_name)\n",
    "    # print(character_list)\n",
    "    df = pd.DataFrame(character_list, columns=[\"Character1\", \"Character2\", \"Distance\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "game_df_bi = create_bigram(game_text_cleaned[:10000], \"A Game of Thrones\")\n",
    "game_df_bi\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from siuba import count\n",
    "\n",
    "# Create our Graph object\n",
    "game_df_bi_graph = count(game_df_bi.head(65), 'Character1','Character2', 'Distance', sort=True)\n",
    "game_df_bi_graph.drop(columns=['Distance'])\n",
    "G = nx.from_pandas_edgelist(df=game_df_bi_graph, source='Character1', target='Character2', edge_attr='n')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Time to draw it\n",
    "nx.draw(G, pos=nx.spring_layout(G, k=1.45, iterations=200), node_size=400, font_size=10, node_color='r', edge_color='b', with_labels=True)\n",
    "figure(figsize=(800, 800))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}